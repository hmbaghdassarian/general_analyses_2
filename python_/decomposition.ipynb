{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expr is a cell x gene matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "pca_op = decomposition.PCA(n_components = n_components, random_state=seed)\n",
    "data_pca = pca_op.fit_transform(expr)\n",
    "\n",
    "# format\n",
    "data_pca = pd.DataFrame(data_pca, \n",
    "            columns = ['PC_' + str(i) for i in range(1,n_components+1)], \n",
    "            index = expr.index)\n",
    "\n",
    "# # here is how explained variance is calculated - \n",
    "# # - analgous to seurat calculation and stored in pca_op.explained_variance_ratio_:\n",
    "\n",
    "eigValues = pca_op.singular_values_**2\n",
    "explained_variance_ = eigValues / (n_samples - 1)\n",
    "total_var = np.var(expr, ddof=1, axis=0).sum() # variance of genes total\n",
    "explained_variance_ratio_ = explained_variance_ / total_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of explained variance\n",
    "thresh = 0.001 #(0.1% of variance explained)\n",
    "ev = pd.DataFrame(data = {'Explained_Variance': pca_op.explained_variance_ratio_, \n",
    "                         'Cumulative_Explained_Variance': np.cumsum(pca_op.explained_variance_ratio_)})\n",
    "ev['Difference'] = ev.Explained_Variance.diff().abs().tolist()\n",
    "n_pcs = n_components # because n_pcs is 7, and 200 components explains only 38% of total variance\n",
    "\n",
    "print('The total PCs (at which additional variance explained is < 0.1%) used is: '.format(n_pcs))\n",
    "\n",
    "# visualize\n",
    "ev = ev.iloc[:n_components, ]\n",
    "ev['PC'] = range(1, n_components + 1)\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sns.scatterplot(data = ev, x = 'PC', y = 'Cumulative_Explained_Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap # umap-learn\n",
    "\n",
    "umap_op = umap.UMAP(n_components = 2, random_state = seed)\n",
    "data_umap = umap_op.fit_transform(data_pca[:n_pcs])\n",
    "data_umap = pd.DataFrame(data_umap, \n",
    "            columns = ['UMAP_' + str(i) for i in range(1,2+1)], \n",
    "            index = expr.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is also a simple sklearn.metrics.normalized_mutual_info_score, but I've hadd issues with it \n",
    "\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import mutual_info_score as MI\n",
    "\n",
    "def entropy1(x): # double check this is correct\n",
    "    value,counts = np.unique(x, return_counts=True)\n",
    "    return entropy(counts, base=None)\n",
    "\n",
    "def NMI(x,y):\n",
    "    return MI(x,y)/(entropy1(x) + entropy1(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:FDA_collab] *",
   "language": "python",
   "name": "conda-env-FDA_collab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
