{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdf1765",
   "metadata": {},
   "source": [
    "Example code based on torchvision from [learnpytorch](https://www.learnpytorch.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3465649",
   "metadata": {},
   "source": [
    "# data_setup\n",
    "\n",
    "a file to prepare and download data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d67417",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 32 # mini-batches\n",
    "RANDOM_SEED = 888"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a7147",
   "metadata": {},
   "source": [
    "Create a custom Dataset class with the commented 5 requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f21937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = self.find_classes(targ_dir)\n",
    "        \n",
    "    # Make function to find classes in target directory\n",
    "    @staticmethod\n",
    "    def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "        \"\"\"Finds the class folder names in a target directory.\n",
    "\n",
    "        Assumes target directory is in standard image classification format.\n",
    "\n",
    "        Args:\n",
    "            directory (str): target directory to load classnames from.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "\n",
    "        Example:\n",
    "            find_classes(\"food_images/train\")\n",
    "            >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "        \"\"\"\n",
    "        # 1. Get the class names by scanning the target directory\n",
    "        classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "        # 2. Raise an error if class names not found\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "\n",
    "        # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac1537",
   "metadata": {},
   "source": [
    "Alternatively, load all data and split into train/test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d12555",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=RANDOM_SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be60332",
   "metadata": {},
   "source": [
    "Specify the custom transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01095f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the transformation\n",
    "custom_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a01b0",
   "metadata": {},
   "source": [
    "Usage:\n",
    "\n",
    "Use the Dataset class to load in data with the transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbb409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data from specified directories\n",
    "train_data_custom = ImageFolderCustom(targ_dir=train_dir, \n",
    "                                      transform=train_transforms)\n",
    "test_data_custom = ImageFolderCustom(targ_dir=test_dir, \n",
    "                                     transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b8d29",
   "metadata": {},
   "source": [
    "Usage:\n",
    "\n",
    "Transform the Dataset into a DataLoader to be used in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn train and test custom Dataset's into DataLoader's\n",
    "train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n",
    "                                     batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                                     num_workers=NUM_WORKERS, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    num_workers=NUM_WORKERS, \n",
    "                                    shuffle=False) # don't usually need to shuffle testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71499ad",
   "metadata": {},
   "source": [
    "# model.py\n",
    "a file to create a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c14a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f978ac",
   "metadata": {},
   "source": [
    "Create the model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Build model\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, hidden_units=8):\n",
    "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
    "\n",
    "        Args:\n",
    "            input_shape (int): Number of input features to the model.\n",
    "            out_shape (int): Number of output features of the model\n",
    "              (how many classes there are).\n",
    "            hidden_units (int): Number of hidden units between layers, default 8.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # flatten inputs into single vector\n",
    "            nn.Linear(in_shape=input_shape, out_shape=hidden_units),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_shape=hidden_units, out_shape=hidden_units),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_shape=hidden_units, out_shape=output_shape), # how many classes are there?\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95685c82",
   "metadata": {},
   "source": [
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of BlobModel and send it to the target device\n",
    "model_instance = CustomModel(input_shape=IN_SHAPE, \n",
    "                            output_shape=OUT_SHAPE, \n",
    "                            hidden_units=8).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d78c75",
   "metadata": {},
   "source": [
    "# engine.py \n",
    "a file to leverage all other files and train a target PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10efbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "EPOCHS = 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3f054",
   "metadata": {},
   "source": [
    "One eopoch of training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to training mode and then\n",
    "    runs through all of the required training steps (forward\n",
    "    pass, loss calculation, optimizer step).\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "    \"\"\"\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9ff44",
   "metadata": {},
   "source": [
    "One epoch of test loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4de38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f06bf4",
   "metadata": {},
   "source": [
    "Iterate across epochs with a specified loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device, \n",
    "         writer: torch.utils.tensorboard.writer.SummaryWriter) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "    writer: A SummaryWriter() instance to log model results to.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "                  train_acc: [...],\n",
    "                  test_loss: [...],\n",
    "                  test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "                 {train_loss: [2.0616, 1.0537],\n",
    "                  train_acc: [0.3945, 0.3945],\n",
    "                  test_loss: [1.2641, 1.5706],\n",
    "                  test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Create a writer with all default settings\n",
    "    writer = create_writer() # function from utility\n",
    "    \n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        \n",
    "        \n",
    "### EXPERIMENT TRACKING########\n",
    "        ### New: Use the writer parameter to track experiments ###\n",
    "        # See if there's a writer, if so, log to it\n",
    "        if writer:\n",
    "            # Add results to SummaryWriter\n",
    "            writer.add_scalars(main_tag=\"Loss\", \n",
    "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                                \"test_loss\": test_loss},\n",
    "                               global_step=epoch)\n",
    "            writer.add_scalars(main_tag=\"Accuracy\", \n",
    "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                                \"test_acc\": test_acc}, \n",
    "                               global_step=epoch)\n",
    "\n",
    "            # Close the writer\n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5702c5",
   "metadata": {},
   "source": [
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_instance.parameters(), \n",
    "                             lr=0.1)\n",
    "writer = create_writer(...) # from utility function\n",
    "\n",
    "# train\n",
    "results = train(model: model_instance, \n",
    "                train_dataloader = train_dataloader_custom, \n",
    "                test_dataloader = test_dataloader_custom, \n",
    "                optimizer = optimizer,\n",
    "                loss_fn = loss_fn,\n",
    "                epochs = EPOCHS,\n",
    "                device = device, \n",
    "               writer = writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a48a4",
   "metadata": {},
   "source": [
    "Vies results from SummaryWriter using tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4e4d8",
   "metadata": {},
   "source": [
    "# train.py \n",
    "a file to leverage all other files and train a target PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29bb68",
   "metadata": {},
   "source": [
    "# utils.py \n",
    "a file dedicated to helpful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78499243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "    \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "    Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "    state_dict: save entire model or the state dict\n",
    "\n",
    "    Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "    \"\"\"\n",
    "    # Create target directory\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # Save the model state_dict()\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60feefa2",
   "metadata": {},
   "source": [
    "Helper function to create summary writer instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1af4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def create_writer(experiment_name: str, \n",
    "                  model_name: str, \n",
    "                  extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():\n",
    "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
    "\n",
    "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
    "\n",
    "    Where timestamp is the current date in YYYY-MM-DD format.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of experiment.\n",
    "        model_name (str): Name of model.\n",
    "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
    "\n",
    "    Example usage:\n",
    "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
    "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                               model_name=\"effnetb2\",\n",
    "                               extra=\"5_epochs\")\n",
    "        # The above is the same as:\n",
    "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "        \n",
    "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1004f",
   "metadata": {},
   "source": [
    "Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_instance, 'path/to/', 'model.pth')\n",
    "\n",
    "model_instance_2 = CustomModel(input_shape=IN_SHAPE, \n",
    "                            output_shape=OUT_SHAPE, \n",
    "                            hidden_units=8).to(device)\n",
    "\n",
    "# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\n",
    "model_instance_2.load_state_dict(torch.load(f='path/to/model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_practice]",
   "language": "python",
   "name": "conda-env-ML_practice-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
